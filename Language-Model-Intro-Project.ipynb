{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "This project focuses on a simple but effective type of language model (LM) called N-gram models. Given a sentence, s, we can construct a list of n-grams from s by finding pairs of words that occur next to each other. From this, we can count the number of occurrences of each n-gram. This count determines the frequency with which an n-gram occurs throughout our document.\n",
    "\n",
    "1. Write code to download the text of a book from a url, returning the corpus as a string.\n",
    "1. Tokenize the corpus into sequences of words and paragraphs.\n",
    "1. Create two \"baseline\" langunage models; create text with them using sampling.\n",
    "1. Compute an N-gram language model\n",
    "1. Write code to sample from the N-gram language model; create text with them.\n",
    "1. Evaluate performance (computational) of your language model for different N.\n",
    "1. Evaluate performance (statistical) of your language model for different N.\n",
    "1. Create an auto-completion suggestion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import project04 as proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the book(s)\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "For this question, you will use `requests` to download and prepare a public domain book from [Project Gutenberg](). Create a function `get_book` that takes in the `url` of a 'Plain Text UTF-8' book and returns a string containing the contents of the book. \n",
    "\n",
    "The function should satisfy the following conditions:\n",
    "* The contents of the book consist of everything between Project Gutenberg's START and END comments.\n",
    "* The contents *will* include title/author/table of contents.\n",
    "* You should also transform any Windows new-lines (`\\r\\n`) with standard new-lines (`\\n`).\n",
    "* You should check the site's `robots.txt` as well and implement a 'pause' in your request in accordance with the website's policy. If the function is called twice in succession, it should not violate the `robots.txt` policy.\n",
    "\n",
    "*Note:* You are encouraged to find whatever books on the website that interest you to test your code and the language models you develop. The text doesn't even need to be an English-language book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.gutenberg.org/files/57988/57988-0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw = requests.get(url, timeout=5).text\n",
    "# Discards the metadata from the beginning of the book\n",
    "start = re.search(r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK .* \\*\\*\\*\",raw ).end()\n",
    "# Discards the metadata from the end of the book\n",
    "stop = re.search(r\"\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK .* \\*\\*\\*\", raw).start()\n",
    "# Keeps the relevant text\n",
    "text = raw[start:stop]\n",
    "text = re.sub('\\r\\n', '\\n', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the actual book text\n",
    "url = 'http://www.gutenberg.org/files/74/74-0.txt'\n",
    "book_string = proj.get_book(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Corpus\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Now you need to tokenize the corpus in `book_string` (turn the text into a list of tokens -- i.e. words and punctuation). More specifically, create a function `tokenize` that takes in `book_string` and outputs a list of tokens satisfying the following conditions:\n",
    "* The start of any paragraph should be represented in the list with the single character `\\x02` (standing for START).\n",
    "* The end of any paragraph should be represented in the list with the single character `\\x03` (standing for STOP).\n",
    "* Tokens in the sequence of words are split apart at 'word boundaries' (see the regex lecture).\n",
    "* Tokens should include *no* whitespace.\n",
    "\n",
    "For example, the following sentence (the ellipses denote preceding/continuing text):\n",
    "```\n",
    "...\n",
    "My phone's dead.\n",
    "\n",
    "I didn't get your call.\n",
    "...\n",
    "```\n",
    "should tokenize to:\n",
    "```\n",
    "[ ...\n",
    "\"My\", \"phone\", \"'\", \"s\", \"dead\", \".\", \"\\x03\", \"\\x02\", \"I\", \"did\", \"'\", \"t\", \"get\", \"your\", \"call\", \".\"\n",
    "... ]\n",
    "```\n",
    "\n",
    "*Remark:* Your tokenization function should run quickly; you should avoid loops when possible. Your `tokenize` function should run on the the complete works of Shakespeare (in `data/shakespeare`) in **under 10 seconds** to get full credit. Be sure to only request the book once (using your `get_book` function) and save it to file for testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_fp = os.path.join('data', 'shakespeare.txt')\n",
    "shakespeare = open(shakespeare_fp, encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906 ms ± 4.49 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit proj.tokenize(shakespeare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Language Model\n",
    "\n",
    "Now that you've learned how to (theoretically) build a language model, you have to implement it in code. Every language model you build will be a class with a few methods in common:\n",
    "\n",
    "* The `__init__` constructor: when you instantiate an LM object, you will need to pass the 'training corpus' on which your model will be trained (i.e. a list of tokens you created above with `tokenize`). The `train` method will then use that data to create a model which is saved in the `mdl` attribute. This code is given to you.\n",
    "* The `train` method takes in a list of tokens (e.g. the output of `tokenize`) and outputs a language model. This language model is usually represented as a `Series` (indexed by tokens; values are probabilities that token occurs), or a `DataFrame`.\n",
    "* The `probability` method takes in a sequence of tokens and returns the probability that this sequence occurs under the language model.\n",
    "* The `sample` method takes in a number `N > 0` and generates a string made up of `N` tokens using the language model. This method generates language.\n",
    "\n",
    "You will create three language model classes in the problems below (Uniform; Unigram; N-gram). For each of these, you will implement each of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models: uniform and unigram LMs\n",
    "\n",
    "### Uniform probability language model\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "A uniform language model is one in which any word is equally likely to appear in any position. The starter code contains a class `UniformLM` which represents a uniform language model. Review the specifics of the class methods above (and in the starter-code) and implement the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one      0.25\n",
       "two      0.25\n",
       "three    0.25\n",
       "four     0.25\n",
       "dtype: float64"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = 'Works'\n",
    "counts = ser.value_counts()\n",
    "prob = counts.loc[char] / ser.shape[0]\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(words):\n",
    "        \"\"\"\n",
    "        probability gives the probabiliy a sequence of words\n",
    "        appears under the language model.\n",
    "        :param: words: a tuple of tokens\n",
    "        :returns: the probability `words` appears under the language\n",
    "        model.\n",
    "\n",
    "        :Example:\n",
    "        >>> tokens = tuple('one one two three one two four'.split())\n",
    "        >>> unif = UniformLM(tokens)\n",
    "        >>> unif.probability(('five',))\n",
    "        0\n",
    "        >>> unif.probability(('one', 'two')) == 0.0625\n",
    "        True\n",
    "        \"\"\"\n",
    "        \n",
    "        prob = 1\n",
    "        trainer = self.mdl\n",
    "        for word in words:\n",
    "            if word not in trainer.index:\n",
    "                return 0\n",
    "            else:\n",
    "                prob = prob * trainer.loc[word]\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram language model\n",
    "\n",
    "**Question 4**\n",
    "\n",
    "A unigram language model is one in which the likelihood a word appears is proportional to its occurrence in the text. That is, it's just the (unconditional) empirical distribution of words in the corpus. The starter code contains a class `UnigramLM` which represents a unigram language model. Review the specifics of the class methods above (and in the starter-code) and implement the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = proj.UnigramLM(tokens)\n",
    "unigram.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Baseline models\n",
    "\n",
    "You've now trained two baseline language models capable of generating new text from a given training text. Attempt to answer these questions for yourself before you continue.\n",
    "\n",
    "* Which model do you think is better? Why?\n",
    "* What are the ways in which both of these models are bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Model\n",
    "\n",
    "Now you will build an N-gram language model, which we be a little more involved. The `NGramLM` class contains a few extra methods and attributes beyond those of `UniformLM` and `UnigramLM`:\n",
    "\n",
    "1. Instantiating `NGramLM` requires both a list of tokens and an `N > 0`, specifying the `N` in N-grams. This parameter is stored in an attribute `.N`.\n",
    "1. The `NGramLM` class has a method `create_ngrams` that takes in a list of tokens and returns a list of N-grams (an N-gram is a `tuple` of length `N`). This list of N-grams is then passed to the `train` method to train the N-gram model stored in the `mdl` attribute.\n",
    "1. While the `train` method still creates a language-model (in this case, an N-gram model), this model is most naturally stored as a DataFrame. This DataFrame will have three columns:\n",
    "    - `'ngram'`: containing the n-grams found in the text.\n",
    "    - `'n1gram'`: containing the (n-1)-grams upon which the n-grams in `ngram` are built.\n",
    "    - `'prob'`: containing the probabilities of each n-gram in `ngram`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating n-grams\n",
    "\n",
    "**Question 5**\n",
    "\n",
    "When computing the likelihood that a word occurs in an N-gram model, you compute the probability that word occurs conditional on the (N-1) words that came before it. Therefore, you need to compute a list of n-grams out of your list of tokens (see the introduction for the definition).\n",
    "\n",
    "*Remark*: What if a word doesn't have (N-1) words preceding it? (e.g. it's near the beginning of the text). How do we compute the likelihood of such a word occurs conditional on the (N-1) words that precede it? To handle this corner case, you should repeat the START (`\\x02`) and STOP (`\\x03`) tokens (N-1)-times. Thus, if you want to compute the likelihood of the first word $w=w_1$ in the text, you would compute:\n",
    "$$P(w) = P(w_1) = P(w_1|\\texttt{\\\\x02},\\ldots,\\texttt{\\\\x02})$$\n",
    "\n",
    "Create a method of `NGramLM` called `create_ngrams` that takes in a list of tokens and returns a list of N-grams. The START/STOP tokens in the N-grams should be handled as explained in the remark above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\x02', 'one', 'two', 'three', 'one', 'four', '\\x03')"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple('\\x02 one two three one four \\x03'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = []\n",
    "lst.append(tuple([0] * 3))\n",
    "idx = 0\n",
    "while lst[-1][-1] != (len(tokens) - 1):\n",
    "    tup = list(lst[idx])\n",
    "    last_val = tup[-1]\n",
    "    \n",
    "    lst.append(curr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0),\n",
       " (0, 0, 1),\n",
       " (0, 1, 2),\n",
       " (1, 2, 3),\n",
       " (2, 3, 4),\n",
       " (3, 4, 5),\n",
       " (4, 5, 6),\n",
       " (5, 6, 6),\n",
       " (6, 6, 6)]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = []\n",
    "tup = tuple([0] * 3)\n",
    "lst.append(tup)\n",
    "l = list(lst[0])\n",
    "counter = 0\n",
    "end = False\n",
    "while lst[-1][0] != (len(tokens) - 1):\n",
    "    if counter == len(tokens) -1:\n",
    "        end = True\n",
    "        val = counter\n",
    "    l = l[1:] + [l[0]]\n",
    "    if end == True:\n",
    "        l[-1] = val\n",
    "    else:\n",
    "        l[-1] = l[-2] + 1\n",
    "\n",
    "    lst.append(tuple(l))\n",
    "    counter += 1\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)    1\n",
       "(6, 6, 6)    1\n",
       "(0, 0, 1)    1\n",
       "(2, 3, 4)    1\n",
       "(3, 4, 5)    1\n",
       "(0, 1, 2)    1\n",
       "(4, 5, 6)    1\n",
       "(0, 0, 0)    1\n",
       "(5, 6, 6)    1\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([lst]).transpose()\n",
    "ser = df[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)    2\n",
       "(3, 4)    1\n",
       "(5, 6)    1\n",
       "(4, 5)    1\n",
       "(6, 6)    1\n",
       "(1, 2)    1\n",
       "(2, 3)    1\n",
       "(0, 1)    1\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst2 = [tuple(list(x)[:-1]) for x in lst]\n",
    "df = pd.DataFrame([lst2]).transpose()\n",
    "ser = df[0].value_counts()\n",
    "ser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the N-gram LM\n",
    "\n",
    "**Question 6**\n",
    "\n",
    "Now, you will compute the probabilities that define N-gram language model itself. Recall that the N-gram LM consists of probabilities\n",
    "$$P(w_1,\\ldots,w_N) = P(w_N | w_1,\\ldots, w_{N-1}) = \\frac{C(w_1, \\ldots, w_N)}{C(w_1,\\ldots, w_{N-1})}$$\n",
    "\n",
    "for every n-gram $(w_1, \\ldots, w_N)$ that occurs in the text.\n",
    "\n",
    "Create a method of `NGramLM` called `train` that takes in a list of N-grams and returns a dataframe of conditional probabilities with the following columns:\n",
    "\n",
    "* `ngram` contains each N-gram that occurs in the text,\n",
    "* `n1gram` contains the (N-1) tokens that precede the last token in the N-gram occupying the same row.\n",
    "* `prob` contains the conditional probabilities associated to the N-gram occupying the same row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing probabilities from the N-gram LM\n",
    "\n",
    "**Question 7**\n",
    "\n",
    "Create a method of `NGramLM` called `probability` that takes in a 'sentence' (sequence of tokens) and returns the likelihood of the sentence under the language model (see the introduction of the formulas).\n",
    "\n",
    "*Remark:* What is the probability of a sentence that contains a 'never before seen' word? What about a 'never before seen' (N-1)-gram? Is this reasonable? (Answer: no, it's not reasonable. Fixing this sort of deficiency is a topic in NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 6)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tuple('\\x02 one two three one four \\x03'.split())\n",
    "x = proj.NGramLM(2, tokens)\n",
    "x.ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the N-gram LM\n",
    "\n",
    "**Question 8**\n",
    "\n",
    "As you saw creating the baseline models, sampling from your trained LM provides a way to generate new language. You will code up a `sample` method for `NGramLM` and observe how much better (or worse) the generated language appears when compared to your baseline models.\n",
    "\n",
    "Create a method of `NGramLM` called `sample` that takes in a number `M > 0` and generates a string of M tokens using the language model (not counting the initial START token(s)). It should sample a first token of the form $(\\texttt{\\\\x02},\\ldots,\\texttt{\\\\x02}, w_1)$ from the probabilities in `self.mdl` and continue picking words conditional on the previous choice. Helper functions and recursion will be very helpful.\n",
    "\n",
    "*Remark:* If you find, at any point, your LM has no words to sample, then you should return the STOP token (`\\x03`) and continue until you reach the required length.\n",
    "\n",
    "*(Fun) Remark:* Try generating text using different values of `N`. What are the differences (qualitatively)? Don't be alarmed if your samples generate strange quasi-coherent sentences; it's just randomness!\n",
    "\n",
    "*(Fun) Remark:* Try sampling from an `N`-gram model built on books written in different languages (It doesn't even need to be written using the Latin alphabet!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = proj.NGramLM(3, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ngram.sample(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the next word in a sentence\n",
    "\n",
    "**Question 9**\n",
    "\n",
    "Next, you will build a predictor that predicts the most likely word to follow a given sentence. The predictions will be the *maximum likelihood estimate* given by your N-gram model. Recall that your N-gram model contains the probabilities that a token follows an (n-1)-gram; your predictor will pick the token with the highest probability of occurring. \n",
    "\n",
    "Create a function `predict_next` that takes in an instantiated `NGramLM` object and a list of tokens, and predicts the *most likely token* to follow the list of tokens in the input (according to `NGramLM`).\n",
    "\n",
    "*Remark:* For which N does this predictor work best? (Determine this by getting a feel for the output, we won't precisely answer this question) .\n",
    "\n",
    "*Remark:* What would this function look like if it were to take in `UniformLM` or `UnigramLM`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation (Extra Credit)\n",
    "\n",
    "**Question 10**\n",
    "\n",
    "This question attempts to answer the question \"which choice of `N` is best when creating an N-gram model?\"\n",
    "\n",
    "As you likely noticed when generating text using your N-gram models, the sentences appear to become more coherent as N increases. The naive conclusion would be to make N very large. However, such a choice has a downside that you will investigate.\n",
    "\n",
    "One concern is that as N gets larger, the distribution of N-grams get more *sparse* -- i.e. the collection of n-grams all have counts close to 1. This makes the language model more likely to merely generate text that was already present in the original text, as opposed to generating *new* language (why?). \n",
    "\n",
    "In this question, you will quantify this observation: given an N-gram model and a sample of length M, what percentage of (N+1)-grams in the sample are present in the text that was used to create the model?\n",
    "\n",
    "Create a function `evaluate_models` which takes in a list of tokens and:\n",
    "1. Generates N-gram models for `N=2,3,4,5,6,7,8` from the given list of tokens,\n",
    "2. Creates samples of length 1000 from each model (remove START/STOP tokens from the samples),\n",
    "3. Calculates the proportion of (N+1)-grams in each sample that was already present in the original list of tokens.\n",
    "\n",
    "The function should return the proportions in a list of length 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
